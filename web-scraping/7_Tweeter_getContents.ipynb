{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweeter 스크랩\n",
    "\n",
    "\n",
    "## 필요한 라이브러리 설치\n",
    "* 아나콘다 사용시 다음의 프롬프트 창을 열어 conda 명령어로 설치합니다.\n",
    "* pip 사용시 아래에 있는 명령어를 터미널로 설치합니다.\n",
    "<img src=\"https://i.imgur.com/Sar4gdw.jpg\">\n",
    "\n",
    "### Selenium\n",
    "* `conda install -c anaconda selenium`\n",
    "* [Selenium :: Anaconda Cloud](https://anaconda.org/anaconda/selenium)\n",
    "\n",
    "* pip 사용시 : `pip install selenium`\n",
    "\n",
    "### BeautifulSoup\n",
    "* `conda install -c anaconda beautifulsoup4`\n",
    "* [Beautifulsoup4 :: Anaconda Cloud](https://anaconda.org/anaconda/beautifulsoup4)\n",
    "\n",
    "* pip 사용시 : `pip install beautifulsoup4`\n",
    "\n",
    "### tqdm\n",
    "* `conda install -c conda-forge tqdm`\n",
    "* [tqdm/tqdm: A Fast, Extensible Progress Bar for Python and CLI](https://github.com/tqdm/tqdm)\n",
    "* `pip install tqdm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [1:19:15<00:00,  2.38s/it]\n"
     ]
    }
   ],
   "source": [
    "def get_login(driver, id, pwd):\n",
    "    driver.find_element_by_xpath('//*[@id=\"react-root\"]/div/div/div/main/div/div/div/div[1]/div/a[2]/div/span/span').click()\n",
    "    time.sleep(1)\n",
    "    driver.find_element_by_xpath('//*[@id=\"react-root\"]/div/div/div[2]/main/div/div/div[1]/form/div/div[1]/label/div/div[2]/div/input').send_keys(id)\n",
    "    time.sleep(1)\n",
    "    driver.find_element_by_xpath('//*[@id=\"react-root\"]/div/div/div[2]/main/div/div/div[1]/form/div/div[2]/label/div/div[2]/div/input').send_keys(pwd)\n",
    "    time.sleep(1)\n",
    "    driver.find_element_by_xpath('//*[@id=\"react-root\"]/div/div/div[2]/main/div/div/div[1]/form/div/div[3]/div/div/span/span').click()\n",
    "    time.sleep(3)\n",
    "\n",
    "def get_postContents(driver, keyword, num_posts, interval):\n",
    "\n",
    "    titles = []\n",
    "    contents = []\n",
    "    dates = []\n",
    "    soup_titles = []\n",
    "    soup_contents = []\n",
    "    soup_dates = []\n",
    "    post_df = pd.DataFrame(columns=('title', 'content', 'date'))\n",
    "        \n",
    "    # 지정된 회차 동안 반복하면서 스크롤을 화면 맨 아래로 이동한다.\n",
    "    for i in trange(0, num_posts):\n",
    "              \n",
    "        # 현재 문서 높이를 가져와서 저장\n",
    "        prev_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        # 스크롤을 가장 아래로 내림\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\") \n",
    "\n",
    "        # 페이지 로딩 대기\n",
    "        time.sleep(interval)  \n",
    "            \n",
    "        # 현재 화면 높이를 가져와서 저장한다\n",
    "        curr_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        # 현재 브라우저에 표시되고 있는 소스코드 가져오기\n",
    "        soup = bs(driver.page_source, 'html.parser')\n",
    "\n",
    "        try:\n",
    "#             print(\"필요한 정보만 추려낸다\")\n",
    "            soup = bs(driver.page_source, 'html.parser')\n",
    "            soup_titles = soup.find_all('span', attrs={\"class\": \"css-901oao css-16my406 r-1qd0xha r-ad9z0x r-bcqeeo r-qvutc0\"})\n",
    "            soup_contents = soup.find_all('span', attrs={\"class\": \"css-901oao css-16my406 r-1qd0xha r-ad9z0x r-bcqeeo r-qvutc0\"})\n",
    "            soup_dates = soup.find_all('a', attrs={\"class\": \"r-1re7ezh r-1loqt21 r-1q142lx r-1qd0xha r-a023e6 r-16dba41 r-ad9z0x r-bcqeeo r-3s2u2q r-qvutc0 css-4rbku5 css-18t94o4 css-901oao\"})\n",
    "\n",
    "#             print(\"추려낸 내용을 깔끔하게 '개행문자와 공백 제거' 정리한다\")\n",
    "            for title, content, date in zip(soup_titles, soup_contents, soup_dates):\n",
    "                title = title.get_text().replace(\"\\n\",\"\").strip()\n",
    "                titles.append(title)\n",
    "                content = content.get_text().replace(\"\\n\",\"\").strip()\n",
    "                contents.append(content)\n",
    "                date = date.get_text().replace(\"\\n\",\"\").strip()\n",
    "                dates.append(date)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "#         if ( prev_height == curr_height ):\n",
    "#             pass\n",
    "\n",
    "#     print(\"추려낸 내용을 dataframe 에 저장 한다\")\n",
    "    post_df = pd.DataFrame({ \"title\": titles, \"content\": contents, \"date\": dates })\n",
    "    \n",
    "    return post_df.drop_duplicates()\n",
    "\n",
    "def letsfind(keyword):\n",
    "    \n",
    "    num_posts = 2000\n",
    "    interval = 2\n",
    "\n",
    "    # 검색페이지로 이동한다\n",
    "    url = f\"https://twitter.com/search?q={keyword}%20until%3A2020-06-30%20since%3A2019-07-01&src=typed_query&f=live\"\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "\n",
    "    post_df = get_postContents(driver, keyword, num_posts, interval)\n",
    "    # save to csv\n",
    "    filename = \"./scraps/\" + \"tweeter-scrapped_\" + keyword.replace(\" \",\"\") + \".csv\"   \n",
    "    post_df.to_csv(filename, date_format='%Y%m%d', encoding='utf-8-sig')\n",
    "\n",
    "##############################################################################################\n",
    "# 라이브러리 로드\n",
    "# requests는 작은 웹브라우저로 웹사이트 내용을 가져온다.\n",
    "import requests\n",
    "# BeautifulSoup 을 통해 읽어 온 웹페이지를 파싱한다.\n",
    "from bs4 import BeautifulSoup as bs\n",
    "# 크롤링 후 결과를 데이터프레임 형태로 보기 위해 불러온다.\n",
    "import pandas as pd\n",
    "from pandas import read_excel\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "import datetime as dt\n",
    "import time\n",
    "from tqdm import trange\n",
    "from tqdm import tqdm\n",
    "##############################################################################################\n",
    "\n",
    "# 로컬 PC\n",
    "options = webdriver.ChromeOptions()\n",
    "options.headless = True\n",
    "options.add_argument(\"window-size=1920x1080\") # 가상화면 크기\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.135 Safari/537.36\")\n",
    "\n",
    "# # Docker\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# options = webdriver.ChromeOptions()\n",
    "# options.headless = True\n",
    "# options.add_argument('--headless')\n",
    "# options.add_argument('--no-sandbox')\n",
    "# options.add_argument('--disable-dev-shm-usage')\n",
    "# options.add_argument(\"window-size=1920x1080\") # 가상화면 크기\n",
    "# options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.135 Safari/537.36\")\n",
    "\n",
    "# 크롬 드라이버 로드한다.\n",
    "driver = webdriver.Chrome(options=options)\n",
    "# 암묵적으로 웹 자원 로드를 위해 3초까지 기다려 준다.\n",
    "driver.implicitly_wait(5)\n",
    "# 화면을 최대로 키운다\n",
    "driver.maximize_window()\n",
    "\n",
    "# 메인페이지로 이동한다\n",
    "driver.get('https://twitter.com/')\n",
    "\n",
    "# 로그인 한다\n",
    "id = 'inhwan.jung@gmail.com'\n",
    "pwd = '2020@alpha'\n",
    "get_login(driver, id, pwd)\n",
    "    \n",
    "##################### 뉴스 검색 Main 루프 시작 ##################### \n",
    "# my_sheet = '소비키워드'\n",
    "# file_name = 'deskresearch.xlsx'\n",
    "# df = read_excel(file_name, sheet_name = my_sheet, header = 1) # index_col='번호'\n",
    "# keywords = df['핵심단어'].values.tolist()\n",
    "\n",
    "# 조회할 키워드를 설정한다.\n",
    "keywords = ['외식', '식사', '메뉴', '식당', '레스토랑', '가성비', '트렌드', '배달', '맛집', '비대면']\n",
    "keywords = ['비대면']\n",
    "\n",
    "for keyword in keywords:\n",
    "    letsfind(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
