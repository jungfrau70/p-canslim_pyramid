{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다음 뉴스 크롤링\n",
    "\n",
    "\n",
    "## 필요한 라이브러리 설치\n",
    "* 아나콘다 사용시 다음의 프롬프트 창을 열어 conda 명령어로 설치합니다.\n",
    "* pip 사용시 아래에 있는 명령어를 터미널로 설치합니다.\n",
    "<img src=\"https://i.imgur.com/Sar4gdw.jpg\">\n",
    "\n",
    "### Selenium\n",
    "* `conda install -c anaconda selenium`\n",
    "* [Selenium :: Anaconda Cloud](https://anaconda.org/anaconda/selenium)\n",
    "\n",
    "* pip 사용시 : `pip install selenium`\n",
    "\n",
    "### BeautifulSoup\n",
    "* `conda install -c anaconda beautifulsoup4`\n",
    "* [Beautifulsoup4 :: Anaconda Cloud](https://anaconda.org/anaconda/beautifulsoup4)\n",
    "\n",
    "* pip 사용시 : `pip install beautifulsoup4`\n",
    "\n",
    "### tqdm\n",
    "* `conda install -c conda-forge tqdm`\n",
    "* [tqdm/tqdm: A Fast, Extensible Progress Bar for Python and CLI](https://github.com/tqdm/tqdm)\n",
    "* `pip install tqdm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 250/250 [14:06<00:00,  3.39s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████| 250/250 [14:04<00:00,  3.38s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████| 250/250 [14:05<00:00,  3.38s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████| 250/250 [14:05<00:00,  3.38s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████| 250/250 [14:06<00:00,  3.38s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████| 250/250 [14:02<00:00,  3.37s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████| 250/250 [14:02<00:00,  3.37s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████| 250/250 [13:57<00:00,  3.35s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████| 250/250 [13:59<00:00,  3.36s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████| 250/250 [14:01<00:00,  3.37s/it]\n"
     ]
    }
   ],
   "source": [
    "def get_postContents(keyword, num_posts, interval):\n",
    "\n",
    "    \n",
    "    # 목록을 저장할 빈 리스트\n",
    "    titles = []\n",
    "    contents = []\n",
    "    dates = []\n",
    "    post_df = pd.DataFrame(columns=(\"title\", \"content\", \"date\"))\n",
    "\n",
    "    pnum = (num_posts // 10) + 1\n",
    "    for pnum in trange(1, pnum):\n",
    "\n",
    "        keyword_url = f\"https://search.daum.net/search?w=news&DA=STC&enc=utf8&cluster=y&cluster_page=1&q={keyword}&p={pnum}&sd=20190701000000&ed=20200630235959&period=u\"\n",
    "\n",
    "        try:\n",
    "            driver.get(keyword_url)\n",
    "            time.sleep(interval)\n",
    "\n",
    "            soup = bs(driver.page_source, 'html.parser')\n",
    "            posts = soup.find_all('div', attrs={\"class\":\"wrap_cont\"})\n",
    "\n",
    "            for post in posts:\n",
    "                title = post.find('a', attrs={\"class\":\"f_link_b\"}).get_text().strip()\n",
    "                content = post.find('p', attrs={\"class\":\"f_eb desc\"}).get_text().strip()\n",
    "                date = post.find('span', attrs={\"class\":\"f_nb date\"}).get_text().strip()\n",
    "\n",
    "                titles.append(title)\n",
    "                contents.append(content)\n",
    "                dates.append(date)\n",
    "                \n",
    "#             xpath = '//*[@id=\"newsColl\"]/div[4]/span/span[2]/a[9]'\n",
    "#             page_count = driver.find_element_by_xpath(xpath).get_text()\n",
    "#             print(page_count)\n",
    "#                 print (titles, summarys, dates, sources)   \n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "\n",
    "    post_df = pd.DataFrame({\"title\": titles, \"content\": contents, \"date\": dates})\n",
    "    return post_df\n",
    "\n",
    "\n",
    "def letsfind(keyword):\n",
    "\n",
    "    num_posts = 2500\n",
    "    interval = 3\n",
    "\n",
    "    try:\n",
    "        post_df = get_postContents(keyword, num_posts, interval)\n",
    "        # save to csv\n",
    "        filename = \"./scraps/\" + \"daum-news-scrapped_\" + keyword.replace(\" \",\"\") + \".csv\"   \n",
    "        post_df.to_csv(filename, date_format='%Y%m%d', encoding='utf-8-sig')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# 라이브러리 로드\n",
    "##############################################################################################\n",
    "# BeautifulSoup 을 통해 읽어 온 웹페이지를 파싱한다.\n",
    "from bs4 import BeautifulSoup as bs\n",
    "# 크롤링 후 결과를 데이터프레임 형태로 보기 위해 불러온다.\n",
    "import pandas as pd\n",
    "from pandas import read_excel\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import time\n",
    "from tqdm import trange\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "##############################################################################################\n",
    "\n",
    "# Local\n",
    "options = webdriver.ChromeOptions()\n",
    "options.headless = True\n",
    "options.add_argument(\"window-size=1920x1080\") # 가상화면 크기\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.135 Safari/537.36\")\n",
    "\n",
    "# # Docker\n",
    "# options = webdriver.ChromeOptions()\n",
    "# options.headless = True\n",
    "# options.add_argument('--headless')\n",
    "# options.add_argument('--no-sandbox')\n",
    "# options.add_argument('--disable-dev-shm-usage')\n",
    "# options.add_argument(\"window-size=1920x1080\") # 가상화면 크기\n",
    "# options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.135 Safari/537.36\")\n",
    "\n",
    "# 크롬 드라이버 로드한다.\n",
    "driver = webdriver.Chrome(options=options)\n",
    "# 암묵적으로 웹 자원 로드를 위해 3초까지 기다려 준다.\n",
    "driver.implicitly_wait(5)\n",
    "# 화면을 최대로 키운다\n",
    "driver.maximize_window()\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "keywords = ['외식', '식사', '메뉴', '식당', '레스토랑', '가성비', '트렌드', '배달', '맛집', '비대면']\n",
    "\n",
    "for keyword in keywords:\n",
    "    letsfind(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
