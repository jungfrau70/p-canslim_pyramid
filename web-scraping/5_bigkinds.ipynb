{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigkinds 크롤링\n",
    "\n",
    "\n",
    "## 필요한 라이브러리 설치\n",
    "* 아나콘다 사용시 다음의 프롬프트 창을 열어 conda 명령어로 설치합니다.\n",
    "* pip 사용시 아래에 있는 명령어를 터미널로 설치합니다.\n",
    "<img src=\"https://i.imgur.com/Sar4gdw.jpg\">\n",
    "\n",
    "### Selenium\n",
    "* `conda install -c anaconda selenium`\n",
    "* [Selenium :: Anaconda Cloud](https://anaconda.org/anaconda/selenium)\n",
    "\n",
    "* pip 사용시 : `pip install selenium`\n",
    "\n",
    "### BeautifulSoup\n",
    "* `conda install -c anaconda beautifulsoup4`\n",
    "* [Beautifulsoup4 :: Anaconda Cloud](https://anaconda.org/anaconda/beautifulsoup4)\n",
    "\n",
    "* pip 사용시 : `pip install beautifulsoup4`\n",
    "\n",
    "### tqdm\n",
    "* `conda install -c conda-forge tqdm`\n",
    "* [tqdm/tqdm: A Fast, Extensible Progress Bar for Python and CLI](https://github.com/tqdm/tqdm)\n",
    "* `pip install tqdm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 3702/3702 [4:19:57<00:00,  4.21s/it]\n"
     ]
    }
   ],
   "source": [
    "def get_login(id, pwd):     \n",
    "    driver.find_element_by_xpath('//*[@id=\"app-top-navbar\"]/ul/li[1]/a').click()\n",
    "    time.sleep(1)\n",
    "\n",
    "    driver.find_element_by_xpath('//*[@id=\"login-user-id\"]').send_keys(id)\n",
    "    driver.find_element_by_xpath('//*[@id=\"login-user-password\"]').send_keys(pwd)\n",
    "    driver.find_element_by_xpath('//*[@id=\"login-btn\"]').click()\n",
    "    time.sleep(1)\n",
    "\n",
    "def set_date(startdate, enddate):\n",
    "    driver.find_element_by_xpath('//*[@id=\"date-filter-btn\"]').click()\n",
    "    driver.find_element_by_xpath('//*[@id=\"search-begin-date\"]').send_keys(Keys.CONTROL + \"a\")\n",
    "    driver.find_element_by_xpath('//*[@id=\"search-begin-date\"]').send_keys(Keys.DELETE)\n",
    "    driver.find_element_by_xpath('//*[@id=\"search-begin-date\"]').send_keys(startdate)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    driver.find_element_by_xpath('//*[@id=\"search-end-date\"]').send_keys(Keys.CONTROL + \"a\")\n",
    "    driver.find_element_by_xpath('//*[@id=\"search-end-date\"]').send_keys(Keys.DELETE)    \n",
    "    driver.find_element_by_xpath('//*[@id=\"search-end-date\"]').send_keys(enddate)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    driver.find_element_by_xpath('//*[@id=\"date-confirm-btn\"]').click()\n",
    "    time.sleep(2)\n",
    "\n",
    "    \n",
    "def get_postContents(keyword, num_news, interval):\n",
    "\n",
    "#     keyword_url = base_url + keyword\n",
    "    \n",
    "    titles = []\n",
    "    contents = []\n",
    "    dates = []\n",
    "    soup_titles = []\n",
    "    soup_contents = []\n",
    "    soup_dates = []\n",
    "    post_df = pd.DataFrame(columns=('title', 'content', 'date'))\n",
    "    \n",
    "    # 메인페이지로 이동한다\n",
    "    driver.get('https://www.bigkinds.or.kr/')\n",
    "    \n",
    "    # 조회 기간을 입력한다\n",
    "    startdate='2019-07-01'\n",
    "    enddate='2020-06-30'\n",
    "    set_date(startdate, enddate)\n",
    " \n",
    "#     print(\"keyword 입력하고 검색버튼을 누른다\")\n",
    "    driver.find_element_by_xpath('//*[@id=\"total-search-key\"]').send_keys(keyword)\n",
    "    driver.find_element_by_xpath('//*[@id=\"news-search-form\"]/div/div/div/div[1]/span/button').click()\n",
    "    time.sleep(3)  \n",
    "        \n",
    "    soup = bs(driver.page_source, 'html.parser')\n",
    "    total_cnt_str = soup.find(\"span\", id=\"total-news-cnt\").get_text()\n",
    "    total_cnt = int(total_cnt_str.replace(\",\", \"\"))\n",
    "\n",
    "#     print(\"검색된 전체 뉴스를 스크랩 한다\")\n",
    "    pages = (total_cnt // 10) + 1\n",
    "    for page in trange(0, pages):\n",
    "        \n",
    "#         print(\"page 를 넘길 때 인덱스를 사용한다. 여기서는 7 배수를 사용한다.\")\n",
    "        index = (page % 7) + 4\n",
    "\n",
    "        try:\n",
    "#             print(\"필요한 정보만 추려낸다\")\n",
    "            soup = bs(driver.page_source, 'html.parser')\n",
    "#             news = soup.find(\"div\", id=\"news-results\")\n",
    "            soup_titles = soup.find_all('h4', attrs={\"class\": \"news-item__title news-detail\"})\n",
    "            soup_contents = soup.find_all('div', attrs={\"class\": \"news-item__content news-detail\"})\n",
    "            soup_dates = soup.find_all('span', attrs={\"class\": \"news-item__date\"})\n",
    "\n",
    "#             print(\"해당 페이지로 이동한다\")\n",
    "            xpath = f'//*[@id=\"news-results-pagination\"]/ul/li[{index}]/a'\n",
    "#             print (xpath)\n",
    "            driver.find_element_by_xpath(xpath).click()\n",
    "            time.sleep(interval)   \n",
    "\n",
    "#             print(\"추려낸 내용을 깔끔하게 '개행문자와 공백 제거' 정리한다\")\n",
    "            for title, content, date in zip(soup_titles, soup_contents, soup_dates):\n",
    "                title = title.get_text().replace(\"\\n\",\"\").strip()\n",
    "                titles.append(title)\n",
    "                content = content.get_text().replace(\"\\n\",\"\").strip()\n",
    "                contents.append(content)\n",
    "                date = date.get_text().replace(\"\\n\",\"\").strip()\n",
    "                dates.append(date)\n",
    "        except:\n",
    "            pass\n",
    "                     \n",
    "#         print(titles)\n",
    "            \n",
    "#     print(\"추려낸 내용을 dataframe 에 저장 한다\")\n",
    "    post_df = pd.DataFrame({ \"title\": titles, \"content\": contents, \"date\": dates })\n",
    "    \n",
    "#     print(\"dataframe 를 반환한다\")\n",
    "    return post_df\n",
    "\n",
    "def letsfind(keywords):\n",
    "\n",
    "    # base_url = f\"https://m.kin.naver.com/mobile/search/searchList.nhn?cs=utf8&query=\"\n",
    "    num_news = 50000\n",
    "    interval = 3\n",
    "\n",
    "    try:\n",
    "        post_df = get_postContents(keyword, num_news, interval)\n",
    "        filename = \"./scraps/\" + \"bigkinds_\" + keyword.replace(\" \",\"\") + \".csv\"\n",
    "        post_df.to_csv(filename, date_format='%Y%m%d', encoding='utf-8-sig') \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "##############################################################################################\n",
    "# 라이브러리 로드\n",
    "# requests는 작은 웹브라우저로 웹사이트 내용을 가져온다.\n",
    "import requests\n",
    "# BeautifulSoup 을 통해 읽어 온 웹페이지를 파싱한다.\n",
    "from bs4 import BeautifulSoup as bs\n",
    "# 크롤링 후 결과를 데이터프레임 형태로 보기 위해 불러온다.\n",
    "import pandas as pd\n",
    "from pandas import read_excel\n",
    "from selenium import webdriver\n",
    "# from seleniumrequests import Chrome\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "import datetime as dt\n",
    "import time\n",
    "from tqdm import trange\n",
    "from tqdm import tqdm\n",
    "\n",
    "##############################################################################################\n",
    "\n",
    "# 로컬 PC\n",
    "options = webdriver.ChromeOptions()\n",
    "options.headless = False\n",
    "options.add_argument(\"window-size=1920x1080\") # 가상화면 크기\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.135 Safari/537.36\")\n",
    "\n",
    "# # Docker\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# options = webdriver.ChromeOptions()\n",
    "# options.headless = True\n",
    "# options.add_argument('--headless')\n",
    "# options.add_argument('--no-sandbox')\n",
    "# options.add_argument('--disable-dev-shm-usage')\n",
    "# options.add_argument(\"window-size=1920x1080\") # 가상화면 크기\n",
    "# options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.135 Safari/537.36\")\n",
    "\n",
    "# 크롬 드라이버 로드한다.\n",
    "driver = webdriver.Chrome(options=options)\n",
    "# 암묵적으로 웹 자원 로드를 위해 3초까지 기다려 준다.\n",
    "driver.implicitly_wait(5)\n",
    "# 화면을 최대로 키운다\n",
    "driver.maximize_window()\n",
    "# 메인페이지로 이동한다\n",
    "driver.get('https://www.bigkinds.or.kr/')\n",
    "\n",
    "# 로그인 한다\n",
    "id = 'inhwan.jung@gmail.com'\n",
    "pwd = '2020@alpha'\n",
    "get_login(id, pwd)\n",
    "    \n",
    "# 조회할 키워드를 설정한다.\n",
    "##################### Keyword 루프 시작 ##################### \n",
    "keywords = ['외식', '식사', '메뉴', '식당', '레스토랑', '가성비', '트렌드', '배달', '맛집', '비대면']\n",
    "\n",
    "for keyword in keywords:\n",
    "    letsfind(keyword)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
